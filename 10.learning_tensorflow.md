# learn the course of the MOOC

Course address https://www.icourse163.org/learn/PKU-1002536002?tid=1003797005#/learn/content?type=detail&id=1005355442

## Defination of machine learning

如果一个程序可在任务T上，随着经验E的增加，效果P随之增加， 则这个程序可以从经验中学习

## 张量

就是多维数组（列表）

阶：张量的维数

## 计算图（Graph）

搭建神经网络的计算过程，只搭建，不运算

只描述运算过程，不计算结果

## 会话（Session）

执行计算图中的节点运算

## 参数

 即神经元上和权重矩阵

## 神经网络实现过程

准备数据集，提取特征，作为输入喂给神经网络

搭建NN结构，从输入到输出（先搭建计算图，再用会话执行，前向传播）,使用session.run()时，先将变量初始化，可用tf.global_variables_initializer() 初始化, tf.placeholder() 占位， 定义输入和输出，及各层神经网络和参数

大量特征数据喂给NN， 迭代优化参数（后向传播），损失函数减小， 可用函数tf.train.GradientDescentOptimizer(learning_rate).minimize(loss),  或者 MomentumOptimizer(learning_rate).minimize(loss), 或者 AdamOptimizer(learning_rate).minimize(loss)，定义损失函数和反向传播方法

使用训练好的模型预测和分类

##  神经网络优化

### 激活函数(activation function)

增加网络的区分度，引入非线性

用得较多的：tf.nn.relu() tf.nn.sigmoid() tf.nn.tanh()

层数=隐藏层的层数+1个输出层（输入层无计算功能，不计入）

总参数=总权重+总偏移

输出层不过激活函数

### 优化方向：损失函数loss、学习率learning_rate、滑动平均ema、正刚化regularization

loss函数: Mean squared Error, Cross Entropy（两个概率分布之间的距离）, 自定义

Mean squared Error : tf.reduce_mean(tf.square(计算值-真值)) 

Cross Entropy: -tf.reduce_mean(真值*tf.log(tf.clip_by_value(y,min计算值,max计算值))) 

softmax()：使得n分类的n个输出满足概率分布，用法如下(y为计算值，y_为真值)：
> ce = tf.nn.sparse_sotfmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_,1))
> cem = tf.reduce_mean(ce)

Learning_rate: 每次参数更新的幅度
> w(n+1) = w(n) - learning_rate * grad(loss)
> 大了会出现振荡不收敛，小了收敛太慢
> 指数衰减学习率
>> learning_rate = LEARNING_RATE_BASE * LEARNING_RATE_DECAY **(global_step/learning_rate_step)
>>>learning_rate_step=总样本数/BATCH_SIZE 即多少轮更新一次学习率
>>> global_step = tf.Variable(0,trainable=False) //仅用于计数，不是训练参数，令trainable=False
>>> learning_rate =  tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase = True) //staircase为true时，global_step/learning_rate_step取整数，学习率为阶梯衰减，否则为平滑曲线衰减

滑动平均（影子值）
> 记录了每个参数一段时间内过往值的平均，增加了模型的泛化性
> 针对所有参数，就像给参数加了影子，参数变化，影子缓慢
> 影子 =   衰减率 * 影子 + （1 - 衰减率）* 参数 
>> 影子初值=参数初值
>> 衰减率=min{MOVING_AVERAGE_DECAY，(1+轮数)/(10+轮)}
>>> ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, gobal_step)
>>> ema_op = ema.apply(tf.trainable_variables()) //每次运行，所有待优化参数求滑动平均
>>> with tf.control_dependencies([train_step,ema_op])://将计算ema和训练过程合成为一个节点
>>>     train_op = tf.no_op(name='train')
>>>  ema.average(参数名) //查看某参数的滑动平均值

正则化缓和过拟合
> 过拟合：对已经训练过的数据预测精度高，但从未见过的数据预测结果不准确
> 正则化：在损失函数中引入模型复杂度指标，利用给W加权值，弱化训练数据噪声（一般不正则化b）
>> Loss = loss(计算值与真值) + REGULARIZER * loss(w)//用REGULARIZER超参数给出w在总loss中的比例，即正则化的权
>>> loss(w)有两种求法：
>>> tf.contrib.layers.l1_regularizer(REGULARIZER)(loss)//所有w的绝对值求和
>>> tf.contrib.layers.l2_regularizer(REGULARIZER)(loss)//所有w平方求和
>> 计算好loss(w)后，要用tf.add_to_collection('losses',tf.contrib.layers.l2_regularizer(REGULARIZER)(loss))把计算的值放入集合中
>> 再用loss=cem+tf.add_n(tf.get_collection('losses'))将集合加入原loss函数，构成新的loss函数

## 图形可视化模块(matplotlib)

需要安装 sudo pip3 install matplotlib.pyplot
使用方法：
> import matplotlib.pyplot as plt
> plt.scatter(x,y,c='coloer')
> plt.show()
> xx,yy = np.mgrid[起:止:步长,起:止:步长]
> grid = np.c_[xx.ravel(),yy.reval()]//生成矩阵
> probs = sess.run(y,feed_dict={x:grid})
> probs = probs.reshape(xx.shape)
> plt.contour(x轴坐标值,y轴坐标值,该点的高度,levels=[等高线的高度])
> plt.show()


## TIPS
tf.get_collection("")   从集合中取全部变量，生成一个列表

tf.add_n([])    列表内对应元素相加

tf.cast(x,dtype)    把x转为dtype类型

tf.argmax(x,axis)   返回axis维度中，最大值所在索引号

os.path.join("home","name") 返回home/name

字符串.split()

with tf.Graph().as_default() as g:  其内定义的节点在计算图g中，用于复现已经定义好的神经网络

## 对于模型的操作

保存模型：
-  每运行一定轮数的训练，保存一次模型
- **saver = tf.train.Saver()** //实例化saver对象
- with tf.Session() as sess:
-   xxx.....
-   **saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODE_NAME),global_step=global_step)**
-   xxx.....

加载模型：
- with tf.Session() as sess:
-    **ckpt=tf.train.get_checkpoint_state(MODEL_PATH)**
-    **if ckpt and ckpt.model_checkpoint_path:**
-    **saver.restore(sess,ckpt.model_checkpoint_path)**

实例化可还原滑动平均值的saver
- 此时会把滑动平均值作为赋值给模型中的对应参数
- ema = tf.train.ExponentiaMovingAverage(滑动平均基数)
- ema_restore = ema.variables_to_restore()
- saver = tf.train.Saver(ema_restore)

准确率计算方法
- correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))
- accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32)


## 模型测试

def test(mnist):
    with tf.Graph().as_default() as g:
        定义 x y_ y
        实例化可还原滑动平均值的saver
        计算正确率
        while True:
            with tf.Session() as sess:
                #加载ckpt模型
                 实例化加载模块
                 判断ckpt模型是否存在
                    恢复会话
                    恢复轮数
                    计算准确率
                    输出结果
                如果没有模型：
                    给出提示
                    return

def main():
    mnist = input_data.read_data_sets("./data/",one_hot=True)
    test(mnist)
    
if __name__== '__main__':
    main()


## tfrecords

tfrecords是一种二进制文件，可先将图片和标签制作成此格式文件，再用tfrecords读取可提高内存利用率

用tf.train.Example协议存储训练数据，训练数据的特征用键值对形式表示，如'img_raw':值 'label':值 值是 Byteslist/FloatList/Int64List

用SerializeToString()把数据序列化成字符串存储

> 生成tfrecords文件：
> writer=tf.python_io.TFRecordWriter(tfRecordName) #新建一个writer对象
> for  循环遍历每张图和标签:
>   example=tf.train.Example(features=tf.train.Features(feature={
>       'img_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),'label':tf.train.F
>       eature(int64_list=tf.train.Int64List(value=labels))})) #把每张图都装到example中
>   writer.write(example.SerializeToString()) #序列化example
> writer.close()

> 解析tfrecords文件：
> filename_queue=tf.train.string_input_producer([tfRecord_path])
> reader=tf.TFRecordReader() #新建一个Reader对象
> _,serialized_example=reader.read(filename_queue)
features=tf.parse_single_example(serialized_example,features={'img_raw':tf.FixedLenFeature([],tf.string),'label':tf.FixedLenFeature([10],tf.int64)})

