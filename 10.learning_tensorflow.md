# learn the course of the MOOC

Course address https://www.icourse163.org/learn/PKU-1002536002?tid=1003797005#/learn/content?type=detail&id=1005355442

## Defination of machine learning

如果一个程序可在任务T上，随着经验E的增加，效果P随之增加， 则这个程序可以从经验中学习

## 张量

就是多维数组（列表）

阶：张量的维数

## 计算图（Graph）

搭建神经网络的计算过程，只搭建，不运算

只描述运算过程，不计算结果

## 会话（Session）

执行计算图中的节点运算

## 参数

 即神经元上和权重矩阵

## 神经网络实现过程

准备数据集，提取特征，作为输入喂给神经网络

搭建NN结构，从输入到输出（先搭建计算图，再用会话执行，前向传播）,使用session.run()时，先将变量初始化，可用tf.global_variables_initializer() 初始化, tf.placeholder() 占位， 定义输入和输出，及各层神经网络和参数

大量特征数据喂给NN， 迭代优化参数（后向传播），损失函数减小， 可用函数tf.train.GradientDescentOptimizer(learning_rate).minimize(loss),  或者 MomentumOptimizer(learning_rate).minimize(loss), 或者 AdamOptimizer(learning_rate).minimize(loss)，定义损失函数和反向传播方法

使用训练好的模型预测和分类

##  神经网络优化

### 激活函数(activation function)

增加网络的区分度，引入非线性

用得较多的：tf.nn.relu() tf.nn.sigmoid() tf.nn.tanh()

层数=隐藏层的层数+1个输出层（输入层无计算功能，不计入）

总参数=总权重+总偏移

### 优化方向：损失函数loss、学习率learning_rate、滑动平均ema、正刚化regularization

loss函数: Mean squared Error, Cross Entropy（两个概率分布之间的距离）, 自定义

Mean squared Error : tf.reduce_mean(tf.square(计算值-真值)) 

$mse = 1 \voer n \cdot \Sigma (y -y_)^2$

Cross Entropy: -tf.reduce_mean(真值*tf.log(tf.clip_by_value(y,min计算值,max计算值))) 

$H(y_, y)= - \Sigma y_ * logy$
